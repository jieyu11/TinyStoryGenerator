# Usage: python3 src/modeling/inference.py -c config/model_inference.toml
#
# Config of model inference
# 

output_file = "output/test2.csv"
# -------------------
# ---- The model ----
# -------------------

# lora_peft = true
# model_folder = "models/finetuned/peft_full"

lora_peft = false
model_file = "models/finetuned/distilgpt2.pt"

# base model name
model_name = "distilgpt2"
 
# # use the quantized model
# quantized = true

device = "cpu"

# tokens used for separating the keys in the input. 
starter = "<|startoftext|>"
ender = "<|endoftext|>"

# --------------------------
# ---- Model Generation ----
# --------------------------

# random seed setup in torch:
random_seed = 12345678

# maximum tokens to be generated
max_generated_len = 3000

# set up the hyper-parameters for text generation.
[hyper-parameters]
  # Default 1.0, increase it to get more random results. The value used to
  # module the next token probabilities. The higher it is, more random it can
  # be.
  #
	# temperature = 1.25
	temperature = 1.75
  
  # Default is 50. The number of highest probability vocabulary tokens to keep
  # for top-k-filtering.
  #
	top_k = 250

  # Default 1.0. If set to float < 1, only the most probable tokens with
  # probabilities that add up to top_p or higher are kept for generation.
  #
  top_p =1.0

  # Default 1.0. The parameter for repetition penalty. 1.0 means no penalty.
  # The higher it is the less likely the words would repeat themselves. See this
  # paper (https://arxiv.org/pdf/1909.05858.pdf) for more details.
  #
	repetition_penalty = 1.0

  # Default 1. Exponential penalty to the length. 1.0 means no penalty. Set to
  # values < 1.0 in order to encourage the model to generate shorter sequences,
  # to a value > 1.0 in order to encourage the model to produce longer
  # sequences.
  #
	length_penalty = 1

  # Default 0.
  # If set to int > 0, all ngrams of that size can only occur once.
  #
	no_repeat_ngram_size = 0

  # Default 1.  Number of groups to divide num_beams into in order to ensure
  # diversity among different groups of beams. This paper for more details:
  # https://arxiv.org/pdf/1610.02424.pdf.
  #
	num_beam_groups =1

  # only valid when num_beam_groups is > 1, default 0.0, no penalty This value
  # is subtracted from a beam's score if it generates a token same as any beam
  # from other group at a particular time. Note that diversity_penalty is only
  # effective if group beam search is enabled.
  #
	diversity_penalty =0.0

  # bad words list
  bad_words = ["Damn", "Shit", "Fuck"]
